{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "bert-score==0.3.7\n",
    "chardet==4.0.0\n",
    "click==7.1.2\n",
    "cycler==0.10.0\n",
    "dataclasses\n",
    "datasets==1.3.0\n",
    "dill==0.3.3\n",
    "filelock==3.0.12\n",
    "fsspec==0.8.7\n",
    "gensim==3.8.3\n",
    "huggingface-hub==0.0.2\n",
    "idna==2.10\n",
    "importlib-metadata==3.7.0\n",
    "joblib==1.0.1\n",
    "jsonlines==2.0.0\n",
    "kiwisolver==1.3.1\n",
    "matplotlib==3.3.4\n",
    "multiprocess==0.70.11.1\n",
    "nltk==3.5\n",
    "numpy==1.19.5\n",
    "packaging==20.9\n",
    "pandas==1.1.5\n",
    "pillow==8.1.0\n",
    "pyarrow==3.0.0\n",
    "pyparsing==2.4.7\n",
    "python-dateutil==2.8.1\n",
    "pytz==2021.1\n",
    "regex==2020.11.13\n",
    "requests==2.25.1\n",
    "sacremoses==0.0.43\n",
    "scikit-learn==0.24.1\n",
    "scipy==1.5.4\n",
    "six==1.15.0\n",
    "sklearn==0.0\n",
    "smart-open==4.2.0\n",
    "threadpoolctl==2.1.0\n",
    "tokenizers==0.10.1\n",
    "torch==1.7.1\n",
    "tqdm==4.49.0\n",
    "transformers==4.3.3\n",
    "typing-extensions==3.7.4.3\n",
    "urllib3==1.26.3\n",
    "xxhash==2.0.0\n",
    "zipp==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading SciFact database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://scifact.s3-us-west-2.amazonaws.com/release/latest/data.tar.gz\n",
    "tar -xvf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device \"{device}\"')\n",
    "batch_size = 1\n",
    "gpu_batch = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neutral classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelPredictionDataset(Dataset):\n",
    "    def __init__(self, corpus: str, claims: str):\n",
    "        self.samples = []\n",
    "        corpus = {doc['doc_id']: doc for doc in jsonlines.open(corpus)}\n",
    "        label_encodings = {'CONTRADICT': 0, 'NOT_ENOUGH_INFO': 1, 'SUPPORT': 0}\n",
    "        for claim in jsonlines.open(claims):\n",
    "            if claim['evidence']:\n",
    "                for doc_id, evidence_sets in claim['evidence'].items():\n",
    "                    doc = corpus[int(doc_id)]\n",
    "                    for evidence_set in evidence_sets:\n",
    "                        rationale = [doc['abstract'][i].strip() for i in evidence_set['sentences']]\n",
    "                        self.samples.append({\n",
    "                            'claim': claim['claim'],\n",
    "                            'rationale': ' '.join(rationale),\n",
    "                            'label': label_encodings[evidence_set['label']]\n",
    "                        })\n",
    "                    rationale_idx = {s for es in evidence_sets for s in es['sentences']}\n",
    "                    rationale_sentences = [doc['abstract'][i].strip() for i in sorted(list(rationale_idx))]\n",
    "                    self.samples.append({\n",
    "                        'claim': claim['claim'],\n",
    "                        'rationale': ' '.join(rationale_sentences),\n",
    "                        'label': label_encodings[evidence_sets[0]['label']]\n",
    "                    })\n",
    "                    non_rationale_idx = set(range(len(doc['abstract']))) - rationale_idx\n",
    "                    non_rationale_idx = random.sample(non_rationale_idx,\n",
    "                                                      k=min(random.randint(1, 2), len(non_rationale_idx)))\n",
    "                    non_rationale_sentences = [doc['abstract'][i].strip() for i in sorted(list(non_rationale_idx))]\n",
    "                    self.samples.append({\n",
    "                        'claim': claim['claim'],\n",
    "                        'rationale': ' '.join(non_rationale_sentences),\n",
    "                        'label': label_encodings['NOT_ENOUGH_INFO']\n",
    "                    })\n",
    "            else:\n",
    "                for doc_id in claim['cited_doc_ids']:\n",
    "                    doc = corpus[int(doc_id)]\n",
    "                    non_rationale_idx = random.sample(range(len(doc['abstract'])), k=random.randint(1, 2))\n",
    "                    non_rationale_sentences = [doc['abstract'][i].strip() for i in non_rationale_idx]\n",
    "                    self.samples.append({\n",
    "                        'claim': claim['claim'],\n",
    "                        'rationale': ' '.join(non_rationale_sentences),\n",
    "                        'label': label_encodings['NOT_ENOUGH_INFO']\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def encode(claim: List[str], rationale: List[str]):\n",
    "    encoding = tokenizer(claim, rationale, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in DataLoader(dataset, batch_size=gpu_batch):\n",
    "            input_ids, attention_mask = encode(batch['claim'], batch['rationale'])\n",
    "            logits = model(input_ids.to(device)).logits\n",
    "            targets.extend(batch['label'].float().tolist())\n",
    "            outputs.extend(logits.argmax(dim=1).tolist())\n",
    "    return {\n",
    "        'macro_f1': f1_score(targets, outputs, zero_division=0, average='macro'),\n",
    "        'f1': tuple(f1_score(targets, outputs, zero_division=0, average=None)),\n",
    "        'precision': tuple(precision_score(targets, outputs, zero_division=0, average=None)),\n",
    "        'recall': tuple(recall_score(targets, outputs, zero_division=0, average=None))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = LabelPredictionDataset('./data/corpus.jsonl', './data/claims_train.jsonl')\n",
    "devset = LabelPredictionDataset('./data/corpus.jsonl', './data/claims_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1-mnli\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.bert.parameters(), 'lr': 5e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, 128, 50 * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(50):\n",
    "    model.train()\n",
    "    t = tqdm(DataLoader(trainset, batch_size=gpu_batch, shuffle=True))\n",
    "    for i, batch in enumerate(t):\n",
    "        input_ids, attention_mask = encode(batch['claim'], batch['rationale'])\n",
    "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=batch['label'].long().to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        if (i + 1) % (batch_size // gpu_batch) == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            t.set_description(f'Epoch {e}, iter {i}, loss: {round(loss.item(), 4)}')\n",
    "    scheduler.step()\n",
    "#     Eval\n",
    "    train_score = evaluate(model, trainset)\n",
    "    print(f'Epoch {e} train score:')\n",
    "    print(train_score)\n",
    "    dev_score = evaluate(model, devset)\n",
    "    print(f'Epoch {e} dev score:')\n",
    "    print(dev_score)\n",
    "    # Save\n",
    "    save_path = os.path.join('./saved_models', f'neutral_classifier-epoch-{e}-f1-{int(dev_score[\"macro_f1\"] * 1e4)}')\n",
    "    os.makedirs(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Support classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelPredictionDataset(Dataset):\n",
    "    def __init__(self, corpus: str, claims: str):\n",
    "        self.samples = []\n",
    "        corpus = {doc['doc_id']: doc for doc in jsonlines.open(corpus)}\n",
    "        label_encodings = {'CONTRADICT': 0, 'NOT_ENOUGH_INFO': 0, 'SUPPORT': 1}\n",
    "        for claim in jsonlines.open(claims):\n",
    "            if claim['evidence']:\n",
    "                for doc_id, evidence_sets in claim['evidence'].items():\n",
    "                    doc = corpus[int(doc_id)]\n",
    "                    for evidence_set in evidence_sets:\n",
    "                        rationale = [doc['abstract'][i].strip() for i in evidence_set['sentences']]\n",
    "                        self.samples.append({\n",
    "                            'claim': claim['claim'],\n",
    "                            'rationale': ' '.join(rationale),\n",
    "                            'label': label_encodings[evidence_set['label']]\n",
    "                        })\n",
    "                    rationale_idx = {s for es in evidence_sets for s in es['sentences']}\n",
    "                    rationale_sentences = [doc['abstract'][i].strip() for i in sorted(list(rationale_idx))]\n",
    "                    self.samples.append({\n",
    "                        'claim': claim['claim'],\n",
    "                        'rationale': ' '.join(rationale_sentences),\n",
    "                        'label': label_encodings[evidence_sets[0]['label']]\n",
    "                    })\n",
    "                    non_rationale_idx = set(range(len(doc['abstract']))) - rationale_idx\n",
    "                    non_rationale_idx = random.sample(non_rationale_idx,\n",
    "                                                      k=min(random.randint(1, 2), len(non_rationale_idx)))\n",
    "                    non_rationale_sentences = [doc['abstract'][i].strip() for i in sorted(list(non_rationale_idx))]\n",
    "                    self.samples.append({\n",
    "                        'claim': claim['claim'],\n",
    "                        'rationale': ' '.join(non_rationale_sentences),\n",
    "                        'label': label_encodings['NOT_ENOUGH_INFO']\n",
    "                    })\n",
    "            else:\n",
    "                for doc_id in claim['cited_doc_ids']:\n",
    "                    doc = corpus[int(doc_id)]\n",
    "                    non_rationale_idx = random.sample(range(len(doc['abstract'])), k=random.randint(1, 2))\n",
    "                    non_rationale_sentences = [doc['abstract'][i].strip() for i in non_rationale_idx]\n",
    "                    self.samples.append({\n",
    "                        'claim': claim['claim'],\n",
    "                        'rationale': ' '.join(non_rationale_sentences),\n",
    "                        'label': label_encodings['NOT_ENOUGH_INFO']\n",
    "                    })\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "def encode(claim: List[str], rationale: List[str]):\n",
    "    encoding = tokenizer(claim, rationale, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in DataLoader(dataset, batch_size=1):\n",
    "            input_ids, attention_mask = encode(batch['claim'], batch['rationale'])\n",
    "            logits = model(input_ids.to(device)).logits\n",
    "            targets.extend(batch['label'].float().tolist())\n",
    "            outputs.extend(logits.argmax(dim=1).tolist())\n",
    "    return {\n",
    "        'macro_f1': f1_score(targets, outputs, zero_division=0, average='macro'),\n",
    "        'f1': tuple(f1_score(targets, outputs, zero_division=0, average=None)),\n",
    "        'precision': tuple(precision_score(targets, outputs, zero_division=0, average=None)),\n",
    "        'recall': tuple(recall_score(targets, outputs, zero_division=0, average=None))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = LabelPredictionDataset('./data/corpus.jsonl', './data/claims_train.jsonl')\n",
    "devset = LabelPredictionDataset('./data/corpus.jsonl', './data/claims_dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "            {'params': model.roberta.parameters(), 'lr': 5e-5},\n",
    "            {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "        ])\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, 128, 50 * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(50):\n",
    "    model.train()\n",
    "    t = tqdm(DataLoader(trainset, batch_size=1, shuffle=True))\n",
    "    for i, batch in enumerate(t):\n",
    "        input_ids, attention_mask = encode(batch['claim'], batch['rationale'])\n",
    "        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device), labels=batch['label'].long().to(device))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        if (i + 1) % (1 // 1) == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            t.set_description(f'Epoch {e}, iter {i}, loss: {round(loss.item(), 4)}')\n",
    "    scheduler.step()\n",
    "    checkpoint = {\n",
    "        'epoch': e + 1,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    # Eval\n",
    "    train_score = evaluate(model, trainset)\n",
    "    print(f'Epoch {e} train score:')\n",
    "    print(train_score)\n",
    "    dev_score = evaluate(model, devset)\n",
    "    print(f'Epoch {e} dev score:')\n",
    "    print(dev_score)\n",
    "\n",
    "    # Save\n",
    "    save_path = os.path.join('./saved_models', f'support_classifier-epoch-{e}-f1-{int(dev_score[\"macro_f1\"] * 1e4)}')\n",
    "    os.makedirs(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    model.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
